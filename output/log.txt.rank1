[01/02 23:38:25] detectron2 INFO: Rank of current process: 1. World size: 2
[01/02 23:38:26] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]
numpy                            1.24.4
detectron2                       0.6 @/home/zpp2/ycy/fc-clip/detectron2/detectron2
Compiler                         GCC 7.5
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.9.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version                   520.61.05
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           9.3.0
torchvision                      0.10.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.8.1
-------------------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/02 23:38:26] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['SOLVER.IMS_PER_BATCH', '8'], resume=False)
[01/02 23:38:26] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml:
_BASE_: ../maskformer2_R50_bs16_50ep.yaml
MODEL:
  META_ARCHITECTURE: "FCCLIP"
  SEM_SEG_HEAD:
    NAME: "FCCLIPHead"
  # backbone part.
  BACKBONE:
    NAME: "CLIP"
  WEIGHTS: ""
  PIXEL_MEAN: [122.7709383, 116.7460125, 104.09373615]
  PIXEL_STD: [68.5005327, 66.6321579, 70.32316305]
  FC_CLIP:
    CLIP_MODEL_NAME: "convnext_large_d_320"
    CLIP_PRETRAINED_WEIGHTS: "laion2b_s29b_b131k_ft_soup"
    EMBED_DIM: 768
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  MASK_FORMER:
    NUM_OBJECT_QUERIES: 250
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: True
      PANOPTIC_ON: True
      OBJECT_MASK_THRESHOLD: 0.0

DATASETS:
  TRAIN: ("LandDiscover50K",)
  TEST: ("FLAIR_test",)

[01/02 23:38:26] detectron2.utils.env INFO: Using a generated random seed 26122094
[01/02 23:40:53] detectron2 INFO: Rank of current process: 1. World size: 2
[01/02 23:40:54] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]
numpy                            1.24.4
detectron2                       0.6 @/home/zpp2/ycy/fc-clip/detectron2/detectron2
Compiler                         GCC 7.5
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.9.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version                   520.61.05
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           9.3.0
torchvision                      0.10.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.8.1
-------------------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/02 23:40:54] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['SOLVER.IMS_PER_BATCH', '8'], resume=False)
[01/02 23:40:54] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml:
_BASE_: ../maskformer2_R50_bs16_50ep.yaml
MODEL:
  META_ARCHITECTURE: "FCCLIP"
  SEM_SEG_HEAD:
    NAME: "FCCLIPHead"
  # backbone part.
  BACKBONE:
    NAME: "CLIP"
  WEIGHTS: ""
  PIXEL_MEAN: [122.7709383, 116.7460125, 104.09373615]
  PIXEL_STD: [68.5005327, 66.6321579, 70.32316305]
  FC_CLIP:
    CLIP_MODEL_NAME: "convnext_large_d_320"
    CLIP_PRETRAINED_WEIGHTS: "laion2b_s29b_b131k_ft_soup"
    EMBED_DIM: 768
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  MASK_FORMER:
    NUM_OBJECT_QUERIES: 250
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: True
      PANOPTIC_ON: True
      OBJECT_MASK_THRESHOLD: 0.0

DATASETS:
  TRAIN: ("LandDiscover50K",)
  TEST: ("FLAIR_test",)

[01/02 23:40:54] detectron2.utils.env INFO: Using a generated random seed 54740802
[01/02 23:45:54] detectron2 INFO: Rank of current process: 1. World size: 2
[01/02 23:45:54] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]
numpy                            1.24.4
detectron2                       0.6 @/home/zpp2/ycy/fc-clip/detectron2/detectron2
Compiler                         GCC 7.5
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.9.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version                   520.61.05
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           9.3.0
torchvision                      0.10.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.8.1
-------------------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/02 23:45:54] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['SOLVER.IMS_PER_BATCH', '8'], resume=False)
[01/02 23:45:54] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml:
_BASE_: ../maskformer2_R50_bs16_50ep.yaml
MODEL:
  META_ARCHITECTURE: "FCCLIP"
  SEM_SEG_HEAD:
    NAME: "FCCLIPHead"
  # backbone part.
  BACKBONE:
    NAME: "CLIP"
  WEIGHTS: ""
  PIXEL_MEAN: [122.7709383, 116.7460125, 104.09373615]
  PIXEL_STD: [68.5005327, 66.6321579, 70.32316305]
  FC_CLIP:
    CLIP_MODEL_NAME: "convnext_large_d_320"
    CLIP_PRETRAINED_WEIGHTS: "laion2b_s29b_b131k_ft_soup"
    EMBED_DIM: 768
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  MASK_FORMER:
    NUM_OBJECT_QUERIES: 250
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: True
      PANOPTIC_ON: True
      OBJECT_MASK_THRESHOLD: 0.0

DATASETS:
  TRAIN: ("LandDiscover50K",)
  TEST: ("FLAIR_test",)

[01/02 23:45:54] detectron2.utils.env INFO: Using a generated random seed 54821952
[01/02 23:48:37] detectron2 INFO: Rank of current process: 1. World size: 2
[01/02 23:48:37] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]
numpy                            1.24.4
detectron2                       0.6 @/home/zpp2/ycy/fc-clip/detectron2/detectron2
Compiler                         GCC 7.5
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.9.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version                   520.61.05
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           9.3.0
torchvision                      0.10.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.8.1
-------------------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/02 23:48:37] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['SOLVER.IMS_PER_BATCH', '8'], resume=False)
[01/02 23:48:37] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml:
_BASE_: ../maskformer2_R50_bs16_50ep.yaml
MODEL:
  META_ARCHITECTURE: "FCCLIP"
  SEM_SEG_HEAD:
    NAME: "FCCLIPHead"
  # backbone part.
  BACKBONE:
    NAME: "CLIP"
  WEIGHTS: ""
  PIXEL_MEAN: [122.7709383, 116.7460125, 104.09373615]
  PIXEL_STD: [68.5005327, 66.6321579, 70.32316305]
  FC_CLIP:
    CLIP_MODEL_NAME: "convnext_large_d_320"
    CLIP_PRETRAINED_WEIGHTS: "laion2b_s29b_b131k_ft_soup"
    EMBED_DIM: 768
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  MASK_FORMER:
    NUM_OBJECT_QUERIES: 250
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: True
      PANOPTIC_ON: True
      OBJECT_MASK_THRESHOLD: 0.0

DATASETS:
  TRAIN: ("LandDiscover50K",)
  TEST: ("FLAIR_test",)

[01/02 23:48:37] detectron2.utils.env INFO: Using a generated random seed 38026357
[01/02 23:49:30] detectron2 INFO: Rank of current process: 1. World size: 2
[01/02 23:49:31] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]
numpy                            1.24.4
detectron2                       0.6 @/home/zpp2/ycy/fc-clip/detectron2/detectron2
Compiler                         GCC 7.5
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.9.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version                   520.61.05
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           9.3.0
torchvision                      0.10.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.8.1
-------------------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/02 23:49:31] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['SOLVER.IMS_PER_BATCH', '8'], resume=False)
[01/02 23:49:31] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml:
_BASE_: ../maskformer2_R50_bs16_50ep.yaml
MODEL:
  META_ARCHITECTURE: "FCCLIP"
  SEM_SEG_HEAD:
    NAME: "FCCLIPHead"
  # backbone part.
  BACKBONE:
    NAME: "CLIP"
  WEIGHTS: ""
  PIXEL_MEAN: [122.7709383, 116.7460125, 104.09373615]
  PIXEL_STD: [68.5005327, 66.6321579, 70.32316305]
  FC_CLIP:
    CLIP_MODEL_NAME: "convnext_large_d_320"
    CLIP_PRETRAINED_WEIGHTS: "laion2b_s29b_b131k_ft_soup"
    EMBED_DIM: 768
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  MASK_FORMER:
    NUM_OBJECT_QUERIES: 250
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: True
      PANOPTIC_ON: True
      OBJECT_MASK_THRESHOLD: 0.0

DATASETS:
  TRAIN: ("LandDiscover_50K",)
  TEST: ("FLAIR_test",)

[01/02 23:49:31] detectron2.utils.env INFO: Using a generated random seed 31088029
[01/02 23:50:33] detectron2 INFO: Rank of current process: 1. World size: 2
[01/02 23:50:34] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]
numpy                            1.24.4
detectron2                       0.6 @/home/zpp2/ycy/fc-clip/detectron2/detectron2
Compiler                         GCC 7.5
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.9.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version                   520.61.05
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           9.3.0
torchvision                      0.10.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.8.1
-------------------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/02 23:50:34] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['SOLVER.IMS_PER_BATCH', '8'], resume=False)
[01/02 23:50:34] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml:
_BASE_: ../maskformer2_R50_bs16_50ep.yaml
MODEL:
  META_ARCHITECTURE: "FCCLIP"
  SEM_SEG_HEAD:
    NAME: "FCCLIPHead"
  # backbone part.
  BACKBONE:
    NAME: "CLIP"
  WEIGHTS: ""
  PIXEL_MEAN: [122.7709383, 116.7460125, 104.09373615]
  PIXEL_STD: [68.5005327, 66.6321579, 70.32316305]
  FC_CLIP:
    CLIP_MODEL_NAME: "convnext_large_d_320"
    CLIP_PRETRAINED_WEIGHTS: "laion2b_s29b_b131k_ft_soup"
    EMBED_DIM: 768
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  MASK_FORMER:
    NUM_OBJECT_QUERIES: 250
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: True
      PANOPTIC_ON: True
      OBJECT_MASK_THRESHOLD: 0.0

DATASETS:
  TRAIN: ("LandDiscover_50K",)
  TEST: ("FLAIR_test",)

[01/02 23:50:34] detectron2.utils.env INFO: Using a generated random seed 34531218
[01/02 23:51:15] detectron2 INFO: Rank of current process: 1. World size: 2
[01/02 23:51:16] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]
numpy                            1.24.4
detectron2                       0.6 @/home/zpp2/ycy/fc-clip/detectron2/detectron2
Compiler                         GCC 7.5
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.9.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version                   520.61.05
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           9.3.0
torchvision                      0.10.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.8.1
-------------------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[01/02 23:51:16] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['SOLVER.IMS_PER_BATCH', '8'], resume=False)
[01/02 23:51:16] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml:
_BASE_: ../maskformer2_R50_bs16_50ep.yaml
MODEL:
  META_ARCHITECTURE: "FCCLIP"
  SEM_SEG_HEAD:
    NAME: "FCCLIPHead"
  # backbone part.
  BACKBONE:
    NAME: "CLIP"
  WEIGHTS: ""
  PIXEL_MEAN: [122.7709383, 116.7460125, 104.09373615]
  PIXEL_STD: [68.5005327, 66.6321579, 70.32316305]
  FC_CLIP:
    CLIP_MODEL_NAME: "convnext_large_d_320"
    CLIP_PRETRAINED_WEIGHTS: "laion2b_s29b_b131k_ft_soup"
    EMBED_DIM: 768
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  MASK_FORMER:
    NUM_OBJECT_QUERIES: 250
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: True
      PANOPTIC_ON: True
      OBJECT_MASK_THRESHOLD: 0.0

DATASETS:
  TRAIN: ("LandDiscover_50K",)
  TEST: ("FLAIR_test",)

[01/02 23:51:16] detectron2.utils.env INFO: Using a generated random seed 16455895
[01/02 23:51:28] detectron2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): TimmModel(
        (trunk): ConvNeXt(
          (stem): Sequential(
            (0): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
            (1): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
          )
          (stages): Sequential(
            (0): ConvNeXtStage(
              (downsample): Identity()
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
                  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=192, out_features=768, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=768, out_features=192, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): Identity()
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
                  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=192, out_features=768, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=768, out_features=192, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.003)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
                  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=192, out_features=768, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=768, out_features=192, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.006)
                )
              )
            )
            (1): ConvNeXtStage(
              (downsample): Sequential(
                (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
                (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
              )
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
                  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=384, out_features=1536, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=1536, out_features=384, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.009)
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
                  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=384, out_features=1536, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=1536, out_features=384, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.011)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
                  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=384, out_features=1536, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=1536, out_features=384, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.014)
                )
              )
            )
            (2): ConvNeXtStage(
              (downsample): Sequential(
                (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)
                (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
              )
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.017)
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.020)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.023)
                )
                (3): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.026)
                )
                (4): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.029)
                )
                (5): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.031)
                )
                (6): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.034)
                )
                (7): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.037)
                )
                (8): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.040)
                )
                (9): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.043)
                )
                (10): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.046)
                )
                (11): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.049)
                )
                (12): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.051)
                )
                (13): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.054)
                )
                (14): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.057)
                )
                (15): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.060)
                )
                (16): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.063)
                )
                (17): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.066)
                )
                (18): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.069)
                )
                (19): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.071)
                )
                (20): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.074)
                )
                (21): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.077)
                )
                (22): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.080)
                )
                (23): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.083)
                )
                (24): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.086)
                )
                (25): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.089)
                )
                (26): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.091)
                )
              )
            )
            (3): ConvNeXtStage(
              (downsample): Sequential(
                (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)
                (1): Conv2d(768, 1536, kernel_size=(2, 2), stride=(2, 2))
              )
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)
                  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.094)
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)
                  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.097)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)
                  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.100)
                )
              )
            )
          )
          (norm_pre): Identity()
          (head): NormMlpClassifierHead(
            (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())
            (norm): LayerNorm2d((1536,), eps=1e-06, elementwise_affine=True)
            (flatten): Flatten(start_dim=1, end_dim=-1)
            (pre_logits): Identity()
            (drop): Dropout(p=0.0, inplace=False)
            (fc): Identity()
          )
        )
        (head): Sequential(
          (mlp): Mlp(
            (fc1): Linear(in_features=1536, out_features=1536, bias=True)
            (act): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=1536, out_features=768, bias=False)
            (drop2): Dropout(p=0, inplace=False)
          )
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (1): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (2): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (3): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (4): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (5): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (6): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (7): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (8): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (9): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (10): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (11): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (12): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (13): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (14): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (15): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 768)
      (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=768, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 133
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (mask_pooling): MaskPooling()
  (void_embedding): Embedding(1, 768)
)
[01/02 23:51:28] fcclip.data.dataset_mappers.coco_panoptic_new_baseline_dataset_mapper INFO: [COCOPanopticNewBaselineDatasetMapper] Full TransformGens used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=1024, target_width=1024), FixedSizeCrop(crop_size=(1024, 1024))]
[01/02 23:51:28] detectron2.data.datasets.coco INFO: Loaded 51846 images with semantic segmentation from /home/zpp2/ycy/datasets/LandDiscover50K/TR_Image
[01/02 23:51:28] detectron2.data.build INFO: Using training sampler TrainingSampler
[01/02 23:51:28] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[01/02 23:51:28] detectron2.data.common INFO: Serializing 51846 elements to byte tensors and concatenating them all ...
[01/02 23:51:28] detectron2.data.common INFO: Serialized dataset takes 9.01 MiB
[01/02 23:51:28] detectron2.data.build INFO: Making batched data loader with batch_size=4
[01/02 23:51:28] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from  ...
[01/02 23:51:28] fvcore.common.checkpoint INFO: No checkpoint found. Initializing model from scratch
[01/02 23:51:28] detectron2.engine.train_loop INFO: Starting training from iteration 0
[01/02 23:51:34] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/zpp2/ycy/fc-clip/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/zpp2/ycy/fc-clip/detectron2/detectron2/engine/defaults.py", line 496, in run_step
    self._trainer.run_step()
  File "/home/zpp2/ycy/fc-clip/detectron2/detectron2/engine/train_loop.py", line 493, in run_step
    with autocast(dtype=self.precision):
TypeError: __init__() got an unexpected keyword argument 'dtype'
[01/02 23:51:34] detectron2.engine.hooks INFO: Total training time: 0:00:05 (0:00:00 on hooks)
[04/24 19:04:22] detectron2 INFO: Rank of current process: 1. World size: 2
[04/24 19:04:22] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]
numpy                            1.24.4
detectron2                       0.6 @/home/zpp2/ycy/fc-clip/detectron2/detectron2
Compiler                         GCC 7.5
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.9.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version                   520.61.05
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           9.3.0
torchvision                      0.10.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.8.1
-------------------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/24 19:04:22] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['SOLVER.IMS_PER_BATCH', '8'], resume=False)
[04/24 19:04:22] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml:
_BASE_: ../maskformer2_R50_bs16_50ep.yaml
INPUT: 
  DATASET_MAPPER_NAME: "mask_former_semantic"
MODEL:
  META_ARCHITECTURE: "FCCLIP"
  SEM_SEG_HEAD:
    NAME: "FCCLIPHead"
    NUM_CLASSES: 40
  # backbone part.
  BACKBONE:
    NAME: "CLIP"
  WEIGHTS: ""
  PIXEL_MEAN: [122.7709383, 116.7460125, 104.09373615]
  PIXEL_STD: [68.5005327, 66.6321579, 70.32316305]
  FC_CLIP:
    CLIP_MODEL_NAME: "convnext_large_d_320"
    CLIP_PRETRAINED_WEIGHTS: "laion2b_s29b_b131k_ft_soup"
    EMBED_DIM: 768
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  MASK_FORMER:
    NUM_OBJECT_QUERIES: 250
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OBJECT_MASK_THRESHOLD: 0.0

DATASETS:
  TRAIN: ("LandDiscover_50K",)
  TEST: ("FLAIR_test",)



[04/24 19:04:22] detectron2.utils.env INFO: Using a generated random seed 22910903
[04/24 19:04:32] detectron2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): TimmModel(
        (trunk): ConvNeXt(
          (stem): Sequential(
            (0): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
            (1): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
          )
          (stages): Sequential(
            (0): ConvNeXtStage(
              (downsample): Identity()
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
                  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=192, out_features=768, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=768, out_features=192, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): Identity()
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
                  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=192, out_features=768, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=768, out_features=192, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.003)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
                  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=192, out_features=768, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=768, out_features=192, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.006)
                )
              )
            )
            (1): ConvNeXtStage(
              (downsample): Sequential(
                (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
                (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
              )
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
                  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=384, out_features=1536, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=1536, out_features=384, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.009)
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
                  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=384, out_features=1536, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=1536, out_features=384, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.011)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
                  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=384, out_features=1536, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=1536, out_features=384, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.014)
                )
              )
            )
            (2): ConvNeXtStage(
              (downsample): Sequential(
                (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)
                (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
              )
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.017)
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.020)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.023)
                )
                (3): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.026)
                )
                (4): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.029)
                )
                (5): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.031)
                )
                (6): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.034)
                )
                (7): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.037)
                )
                (8): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.040)
                )
                (9): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.043)
                )
                (10): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.046)
                )
                (11): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.049)
                )
                (12): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.051)
                )
                (13): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.054)
                )
                (14): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.057)
                )
                (15): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.060)
                )
                (16): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.063)
                )
                (17): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.066)
                )
                (18): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.069)
                )
                (19): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.071)
                )
                (20): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.074)
                )
                (21): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.077)
                )
                (22): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.080)
                )
                (23): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.083)
                )
                (24): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.086)
                )
                (25): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.089)
                )
                (26): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.091)
                )
              )
            )
            (3): ConvNeXtStage(
              (downsample): Sequential(
                (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)
                (1): Conv2d(768, 1536, kernel_size=(2, 2), stride=(2, 2))
              )
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)
                  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.094)
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)
                  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.097)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)
                  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.100)
                )
              )
            )
          )
          (norm_pre): Identity()
          (head): NormMlpClassifierHead(
            (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())
            (norm): LayerNorm2d((1536,), eps=1e-06, elementwise_affine=True)
            (flatten): Flatten(start_dim=1, end_dim=-1)
            (pre_logits): Identity()
            (drop): Dropout(p=0.0, inplace=False)
            (fc): Identity()
          )
        )
        (head): Sequential(
          (mlp): Mlp(
            (fc1): Linear(in_features=1536, out_features=1536, bias=True)
            (act): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=1536, out_features=768, bias=False)
            (drop2): Dropout(p=0, inplace=False)
          )
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (1): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (2): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (3): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (4): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (5): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (6): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (7): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (8): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (9): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (10): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (11): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (12): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (13): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (14): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (15): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 768)
      (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=768, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 40
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (mask_pooling): MaskPooling()
  (void_embedding): Embedding(1, 768)
)
[04/24 19:04:32] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[04/24 19:04:32] detectron2.data.datasets.coco INFO: Loaded 51846 images with semantic segmentation from /home/zpp2/ycy/datasets/LandDiscover50K/TR_Image
[04/24 19:04:32] detectron2.data.build INFO: Using training sampler TrainingSampler
[04/24 19:04:32] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/24 19:04:32] detectron2.data.common INFO: Serializing 51846 elements to byte tensors and concatenating them all ...
[04/24 19:04:32] detectron2.data.common INFO: Serialized dataset takes 9.01 MiB
[04/24 19:04:32] detectron2.data.build INFO: Making batched data loader with batch_size=4
[04/24 19:04:32] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from  ...
[04/24 19:04:32] fvcore.common.checkpoint INFO: No checkpoint found. Initializing model from scratch
[04/24 19:04:32] detectron2.engine.train_loop INFO: Starting training from iteration 0
[04/24 19:06:27] detectron2.engine.hooks INFO: Overall training speed: 148 iterations in 0:01:46 (0.7183 s / it)
[04/24 19:06:27] detectron2.engine.hooks INFO: Total training time: 0:01:46 (0:00:00 on hooks)
[04/24 19:20:30] detectron2 INFO: Rank of current process: 1. World size: 2
[04/24 19:20:31] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]
numpy                            1.24.4
detectron2                       0.6 @/home/zpp2/ycy/fc-clip/detectron2/detectron2
Compiler                         GCC 7.5
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.9.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version                   520.61.05
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           9.3.0
torchvision                      0.10.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.8.1
-------------------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/24 19:20:31] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['SOLVER.IMS_PER_BATCH', '8'], resume=False)
[04/24 19:20:31] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml:
_BASE_: ../maskformer2_R50_bs16_50ep.yaml
INPUT: 
  DATASET_MAPPER_NAME: "mask_former_semantic"
MODEL:
  META_ARCHITECTURE: "FCCLIP"
  SEM_SEG_HEAD:
    NAME: "FCCLIPHead"
    NUM_CLASSES: 40
  # backbone part.
  BACKBONE:
    NAME: "CLIP"
  WEIGHTS: ""
  PIXEL_MEAN: [122.7709383, 116.7460125, 104.09373615]
  PIXEL_STD: [68.5005327, 66.6321579, 70.32316305]
  FC_CLIP:
    CLIP_MODEL_NAME: "convnext_large_d_320"
    CLIP_PRETRAINED_WEIGHTS: "laion2b_s29b_b131k_ft_soup"
    EMBED_DIM: 768
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  MASK_FORMER:
    NUM_OBJECT_QUERIES: 250
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OBJECT_MASK_THRESHOLD: 0.0

DATASETS:
  TRAIN: ("LandDiscover_50K",)
  TEST: ("FLAIR_test",)



[04/24 19:20:31] detectron2.utils.env INFO: Using a generated random seed 31471983
[04/24 19:20:40] detectron2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): TimmModel(
        (trunk): ConvNeXt(
          (stem): Sequential(
            (0): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
            (1): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
          )
          (stages): Sequential(
            (0): ConvNeXtStage(
              (downsample): Identity()
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
                  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=192, out_features=768, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=768, out_features=192, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): Identity()
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
                  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=192, out_features=768, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=768, out_features=192, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.003)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
                  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=192, out_features=768, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=768, out_features=192, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.006)
                )
              )
            )
            (1): ConvNeXtStage(
              (downsample): Sequential(
                (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
                (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
              )
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
                  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=384, out_features=1536, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=1536, out_features=384, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.009)
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
                  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=384, out_features=1536, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=1536, out_features=384, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.011)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
                  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=384, out_features=1536, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=1536, out_features=384, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.014)
                )
              )
            )
            (2): ConvNeXtStage(
              (downsample): Sequential(
                (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)
                (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
              )
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.017)
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.020)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.023)
                )
                (3): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.026)
                )
                (4): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.029)
                )
                (5): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.031)
                )
                (6): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.034)
                )
                (7): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.037)
                )
                (8): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.040)
                )
                (9): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.043)
                )
                (10): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.046)
                )
                (11): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.049)
                )
                (12): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.051)
                )
                (13): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.054)
                )
                (14): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.057)
                )
                (15): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.060)
                )
                (16): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.063)
                )
                (17): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.066)
                )
                (18): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.069)
                )
                (19): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.071)
                )
                (20): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.074)
                )
                (21): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.077)
                )
                (22): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.080)
                )
                (23): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.083)
                )
                (24): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.086)
                )
                (25): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.089)
                )
                (26): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.091)
                )
              )
            )
            (3): ConvNeXtStage(
              (downsample): Sequential(
                (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)
                (1): Conv2d(768, 1536, kernel_size=(2, 2), stride=(2, 2))
              )
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)
                  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.094)
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)
                  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.097)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)
                  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.100)
                )
              )
            )
          )
          (norm_pre): Identity()
          (head): NormMlpClassifierHead(
            (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())
            (norm): LayerNorm2d((1536,), eps=1e-06, elementwise_affine=True)
            (flatten): Flatten(start_dim=1, end_dim=-1)
            (pre_logits): Identity()
            (drop): Dropout(p=0.0, inplace=False)
            (fc): Identity()
          )
        )
        (head): Sequential(
          (mlp): Mlp(
            (fc1): Linear(in_features=1536, out_features=1536, bias=True)
            (act): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=1536, out_features=768, bias=False)
            (drop2): Dropout(p=0, inplace=False)
          )
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (1): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (2): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (3): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (4): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (5): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (6): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (7): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (8): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (9): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (10): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (11): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (12): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (13): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (14): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (15): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 768)
      (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=768, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 40
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (mask_pooling): MaskPooling()
  (void_embedding): Embedding(1, 768)
)
[04/24 19:20:40] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[04/24 19:20:41] detectron2.data.datasets.coco INFO: Loaded 51846 images with semantic segmentation from /home/zpp2/ycy/datasets/LandDiscover50K/TR_Image
[04/24 19:20:41] detectron2.data.build INFO: Using training sampler TrainingSampler
[04/24 19:20:41] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/24 19:20:41] detectron2.data.common INFO: Serializing 51846 elements to byte tensors and concatenating them all ...
[04/24 19:20:41] detectron2.data.common INFO: Serialized dataset takes 9.01 MiB
[04/24 19:20:41] detectron2.data.build INFO: Making batched data loader with batch_size=4
[04/24 19:20:41] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from  ...
[04/24 19:20:41] fvcore.common.checkpoint INFO: No checkpoint found. Initializing model from scratch
[04/24 19:20:41] detectron2.engine.train_loop INFO: Starting training from iteration 0
[04/24 19:22:49] detectron2.engine.hooks INFO: Overall training speed: 164 iterations in 0:01:59 (0.7290 s / it)
[04/24 19:22:49] detectron2.engine.hooks INFO: Total training time: 0:01:59 (0:00:00 on hooks)
[04/24 19:26:00] detectron2 INFO: Rank of current process: 1. World size: 2
[04/24 19:26:01] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]
numpy                            1.24.4
detectron2                       0.6 @/home/zpp2/ycy/fc-clip/detectron2/detectron2
Compiler                         GCC 7.5
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.9.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version                   520.61.05
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           9.3.0
torchvision                      0.10.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.8.1
-------------------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/24 19:26:01] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['SOLVER.IMS_PER_BATCH', '2'], resume=False)
[04/24 19:26:01] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml:
_BASE_: ../maskformer2_R50_bs16_50ep.yaml
INPUT: 
  DATASET_MAPPER_NAME: "mask_former_semantic"
MODEL:
  META_ARCHITECTURE: "FCCLIP"
  SEM_SEG_HEAD:
    NAME: "FCCLIPHead"
    NUM_CLASSES: 40
  # backbone part.
  BACKBONE:
    NAME: "CLIP"
  WEIGHTS: ""
  PIXEL_MEAN: [122.7709383, 116.7460125, 104.09373615]
  PIXEL_STD: [68.5005327, 66.6321579, 70.32316305]
  FC_CLIP:
    CLIP_MODEL_NAME: "convnext_large_d_320"
    CLIP_PRETRAINED_WEIGHTS: "laion2b_s29b_b131k_ft_soup"
    EMBED_DIM: 768
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  MASK_FORMER:
    NUM_OBJECT_QUERIES: 250
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OBJECT_MASK_THRESHOLD: 0.0

DATASETS:
  TRAIN: ("LandDiscover_50K",)
  TEST: ("FLAIR_test",)



[04/24 19:26:01] detectron2.utils.env INFO: Using a generated random seed 1171948
[04/24 19:26:10] detectron2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): TimmModel(
        (trunk): ConvNeXt(
          (stem): Sequential(
            (0): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
            (1): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
          )
          (stages): Sequential(
            (0): ConvNeXtStage(
              (downsample): Identity()
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
                  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=192, out_features=768, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=768, out_features=192, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): Identity()
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
                  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=192, out_features=768, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=768, out_features=192, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.003)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
                  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=192, out_features=768, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=768, out_features=192, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.006)
                )
              )
            )
            (1): ConvNeXtStage(
              (downsample): Sequential(
                (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
                (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
              )
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
                  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=384, out_features=1536, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=1536, out_features=384, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.009)
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
                  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=384, out_features=1536, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=1536, out_features=384, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.011)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
                  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=384, out_features=1536, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=1536, out_features=384, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.014)
                )
              )
            )
            (2): ConvNeXtStage(
              (downsample): Sequential(
                (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)
                (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
              )
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.017)
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.020)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.023)
                )
                (3): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.026)
                )
                (4): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.029)
                )
                (5): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.031)
                )
                (6): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.034)
                )
                (7): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.037)
                )
                (8): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.040)
                )
                (9): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.043)
                )
                (10): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.046)
                )
                (11): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.049)
                )
                (12): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.051)
                )
                (13): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.054)
                )
                (14): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.057)
                )
                (15): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.060)
                )
                (16): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.063)
                )
                (17): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.066)
                )
                (18): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.069)
                )
                (19): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.071)
                )
                (20): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.074)
                )
                (21): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.077)
                )
                (22): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.080)
                )
                (23): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.083)
                )
                (24): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.086)
                )
                (25): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.089)
                )
                (26): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.091)
                )
              )
            )
            (3): ConvNeXtStage(
              (downsample): Sequential(
                (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)
                (1): Conv2d(768, 1536, kernel_size=(2, 2), stride=(2, 2))
              )
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)
                  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.094)
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)
                  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.097)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)
                  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.100)
                )
              )
            )
          )
          (norm_pre): Identity()
          (head): NormMlpClassifierHead(
            (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())
            (norm): LayerNorm2d((1536,), eps=1e-06, elementwise_affine=True)
            (flatten): Flatten(start_dim=1, end_dim=-1)
            (pre_logits): Identity()
            (drop): Dropout(p=0.0, inplace=False)
            (fc): Identity()
          )
        )
        (head): Sequential(
          (mlp): Mlp(
            (fc1): Linear(in_features=1536, out_features=1536, bias=True)
            (act): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=1536, out_features=768, bias=False)
            (drop2): Dropout(p=0, inplace=False)
          )
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (1): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (2): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (3): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (4): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (5): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (6): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (7): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (8): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (9): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (10): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (11): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (12): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (13): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (14): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (15): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 768)
      (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=768, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 40
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (mask_pooling): MaskPooling()
  (void_embedding): Embedding(1, 768)
)
[04/24 19:26:10] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[04/24 19:26:11] detectron2.data.datasets.coco INFO: Loaded 51846 images with semantic segmentation from /home/zpp2/ycy/datasets/LandDiscover50K/TR_Image
[04/24 19:26:11] detectron2.data.build INFO: Using training sampler TrainingSampler
[04/24 19:26:11] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/24 19:26:11] detectron2.data.common INFO: Serializing 51846 elements to byte tensors and concatenating them all ...
[04/24 19:26:11] detectron2.data.common INFO: Serialized dataset takes 9.01 MiB
[04/24 19:26:11] detectron2.data.build INFO: Making batched data loader with batch_size=1
[04/24 19:26:11] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from  ...
[04/24 19:26:11] fvcore.common.checkpoint INFO: No checkpoint found. Initializing model from scratch
[04/24 19:26:11] detectron2.engine.train_loop INFO: Starting training from iteration 0
[04/24 19:32:36] detectron2.engine.hooks INFO: Overall training speed: 1522 iterations in 0:06:17 (0.2482 s / it)
[04/24 19:32:36] detectron2.engine.hooks INFO: Total training time: 0:06:18 (0:00:01 on hooks)
[04/24 19:32:49] detectron2 INFO: Rank of current process: 1. World size: 2
[04/24 19:32:50] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]
numpy                            1.24.4
detectron2                       0.6 @/home/zpp2/ycy/fc-clip/detectron2/detectron2
Compiler                         GCC 7.5
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.9.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version                   520.61.05
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           9.3.0
torchvision                      0.10.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.8.1
-------------------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/24 19:32:50] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['SOLVER.IMS_PER_BATCH', '12'], resume=False)
[04/24 19:32:50] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml:
_BASE_: ../maskformer2_R50_bs16_50ep.yaml
INPUT: 
  DATASET_MAPPER_NAME: "mask_former_semantic"
MODEL:
  META_ARCHITECTURE: "FCCLIP"
  SEM_SEG_HEAD:
    NAME: "FCCLIPHead"
    NUM_CLASSES: 40
  # backbone part.
  BACKBONE:
    NAME: "CLIP"
  WEIGHTS: ""
  PIXEL_MEAN: [122.7709383, 116.7460125, 104.09373615]
  PIXEL_STD: [68.5005327, 66.6321579, 70.32316305]
  FC_CLIP:
    CLIP_MODEL_NAME: "convnext_large_d_320"
    CLIP_PRETRAINED_WEIGHTS: "laion2b_s29b_b131k_ft_soup"
    EMBED_DIM: 768
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  MASK_FORMER:
    NUM_OBJECT_QUERIES: 250
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OBJECT_MASK_THRESHOLD: 0.0

DATASETS:
  TRAIN: ("LandDiscover_50K",)
  TEST: ("FLAIR_test",)



[04/24 19:32:50] detectron2.utils.env INFO: Using a generated random seed 50247440
[04/24 19:32:59] detectron2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): TimmModel(
        (trunk): ConvNeXt(
          (stem): Sequential(
            (0): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
            (1): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
          )
          (stages): Sequential(
            (0): ConvNeXtStage(
              (downsample): Identity()
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
                  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=192, out_features=768, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=768, out_features=192, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): Identity()
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
                  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=192, out_features=768, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=768, out_features=192, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.003)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
                  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=192, out_features=768, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=768, out_features=192, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.006)
                )
              )
            )
            (1): ConvNeXtStage(
              (downsample): Sequential(
                (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
                (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
              )
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
                  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=384, out_features=1536, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=1536, out_features=384, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.009)
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
                  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=384, out_features=1536, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=1536, out_features=384, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.011)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
                  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=384, out_features=1536, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=1536, out_features=384, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.014)
                )
              )
            )
            (2): ConvNeXtStage(
              (downsample): Sequential(
                (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)
                (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
              )
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.017)
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.020)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.023)
                )
                (3): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.026)
                )
                (4): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.029)
                )
                (5): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.031)
                )
                (6): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.034)
                )
                (7): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.037)
                )
                (8): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.040)
                )
                (9): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.043)
                )
                (10): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.046)
                )
                (11): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.049)
                )
                (12): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.051)
                )
                (13): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.054)
                )
                (14): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.057)
                )
                (15): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.060)
                )
                (16): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.063)
                )
                (17): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.066)
                )
                (18): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.069)
                )
                (19): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.071)
                )
                (20): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.074)
                )
                (21): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.077)
                )
                (22): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.080)
                )
                (23): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.083)
                )
                (24): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.086)
                )
                (25): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.089)
                )
                (26): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.091)
                )
              )
            )
            (3): ConvNeXtStage(
              (downsample): Sequential(
                (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)
                (1): Conv2d(768, 1536, kernel_size=(2, 2), stride=(2, 2))
              )
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)
                  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.094)
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)
                  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.097)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)
                  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.100)
                )
              )
            )
          )
          (norm_pre): Identity()
          (head): NormMlpClassifierHead(
            (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())
            (norm): LayerNorm2d((1536,), eps=1e-06, elementwise_affine=True)
            (flatten): Flatten(start_dim=1, end_dim=-1)
            (pre_logits): Identity()
            (drop): Dropout(p=0.0, inplace=False)
            (fc): Identity()
          )
        )
        (head): Sequential(
          (mlp): Mlp(
            (fc1): Linear(in_features=1536, out_features=1536, bias=True)
            (act): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=1536, out_features=768, bias=False)
            (drop2): Dropout(p=0, inplace=False)
          )
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (1): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (2): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (3): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (4): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (5): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (6): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (7): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (8): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (9): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (10): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (11): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (12): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (13): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (14): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (15): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 768)
      (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=768, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 40
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (mask_pooling): MaskPooling()
  (void_embedding): Embedding(1, 768)
)
[04/24 19:32:59] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[04/24 19:32:59] detectron2.data.datasets.coco INFO: Loaded 51846 images with semantic segmentation from /home/zpp2/ycy/datasets/LandDiscover50K/TR_Image
[04/24 19:32:59] detectron2.data.build INFO: Using training sampler TrainingSampler
[04/24 19:32:59] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/24 19:32:59] detectron2.data.common INFO: Serializing 51846 elements to byte tensors and concatenating them all ...
[04/24 19:32:59] detectron2.data.common INFO: Serialized dataset takes 9.01 MiB
[04/24 19:32:59] detectron2.data.build INFO: Making batched data loader with batch_size=6
[04/24 19:32:59] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from  ...
[04/24 19:32:59] fvcore.common.checkpoint INFO: No checkpoint found. Initializing model from scratch
[04/24 19:32:59] detectron2.engine.train_loop INFO: Starting training from iteration 0
[04/24 19:33:07] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/zpp2/ycy/fc-clip/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/zpp2/ycy/fc-clip/detectron2/detectron2/engine/defaults.py", line 496, in run_step
    self._trainer.run_step()
  File "/home/zpp2/ycy/fc-clip/detectron2/detectron2/engine/train_loop.py", line 506, in run_step
    self.grad_scaler.scale(losses).backward()
  File "/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torch/_tensor.py", line 255, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torch/autograd/__init__.py", line 147, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 458.00 MiB (GPU 1; 23.70 GiB total capacity; 21.12 GiB already allocated; 23.81 MiB free; 22.18 GiB reserved in total by PyTorch)
[04/24 19:33:07] detectron2.engine.hooks INFO: Total training time: 0:00:07 (0:00:00 on hooks)
[04/24 19:33:19] detectron2 INFO: Rank of current process: 1. World size: 2
[04/24 19:33:20] detectron2 INFO: Environment info:
-------------------------------  --------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]
numpy                            1.24.4
detectron2                       0.6 @/home/zpp2/ycy/fc-clip/detectron2/detectron2
Compiler                         GCC 7.5
CUDA compiler                    CUDA 11.8
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.9.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce RTX 3090 (arch=8.6)
Driver version                   520.61.05
CUDA_HOME                        /usr/local/cuda-11.8
Pillow                           9.3.0
torchvision                      0.10.0 @/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.8.1
-------------------------------  --------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.1.2 (Git Hash 98be7e8afa711dc9b66c8ff3504129cb82013cdb)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.1
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.0.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.9.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[04/24 19:33:20] detectron2 INFO: Command line arguments: Namespace(config_file='configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=False, machine_rank=0, num_gpus=2, num_machines=1, opts=['SOLVER.IMS_PER_BATCH', '10'], resume=False)
[04/24 19:33:20] detectron2 INFO: Contents of args.config_file=configs/coco/panoptic-segmentation/fcclip/fcclip_convnext_large_eval_landdiscover50k.yaml:
_BASE_: ../maskformer2_R50_bs16_50ep.yaml
INPUT: 
  DATASET_MAPPER_NAME: "mask_former_semantic"
MODEL:
  META_ARCHITECTURE: "FCCLIP"
  SEM_SEG_HEAD:
    NAME: "FCCLIPHead"
    NUM_CLASSES: 40
  # backbone part.
  BACKBONE:
    NAME: "CLIP"
  WEIGHTS: ""
  PIXEL_MEAN: [122.7709383, 116.7460125, 104.09373615]
  PIXEL_STD: [68.5005327, 66.6321579, 70.32316305]
  FC_CLIP:
    CLIP_MODEL_NAME: "convnext_large_d_320"
    CLIP_PRETRAINED_WEIGHTS: "laion2b_s29b_b131k_ft_soup"
    EMBED_DIM: 768
    GEOMETRIC_ENSEMBLE_ALPHA: 0.4
    GEOMETRIC_ENSEMBLE_BETA: 0.8
  MASK_FORMER:
    NUM_OBJECT_QUERIES: 250
    TEST:
      SEMANTIC_ON: True
      INSTANCE_ON: False
      PANOPTIC_ON: False
      OBJECT_MASK_THRESHOLD: 0.0

DATASETS:
  TRAIN: ("LandDiscover_50K",)
  TEST: ("FLAIR_test",)



[04/24 19:33:20] detectron2.utils.env INFO: Using a generated random seed 20438136
[04/24 19:33:29] detectron2.engine.defaults INFO: Model:
FCCLIP(
  (backbone): CLIP(
    (clip_model): CLIP(
      (visual): TimmModel(
        (trunk): ConvNeXt(
          (stem): Sequential(
            (0): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
            (1): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
          )
          (stages): Sequential(
            (0): ConvNeXtStage(
              (downsample): Identity()
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
                  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=192, out_features=768, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=768, out_features=192, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): Identity()
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
                  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=192, out_features=768, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=768, out_features=192, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.003)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
                  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=192, out_features=768, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=768, out_features=192, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.006)
                )
              )
            )
            (1): ConvNeXtStage(
              (downsample): Sequential(
                (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
                (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
              )
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
                  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=384, out_features=1536, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=1536, out_features=384, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.009)
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
                  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=384, out_features=1536, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=1536, out_features=384, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.011)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
                  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=384, out_features=1536, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=1536, out_features=384, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.014)
                )
              )
            )
            (2): ConvNeXtStage(
              (downsample): Sequential(
                (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)
                (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
              )
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.017)
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.020)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.023)
                )
                (3): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.026)
                )
                (4): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.029)
                )
                (5): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.031)
                )
                (6): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.034)
                )
                (7): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.037)
                )
                (8): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.040)
                )
                (9): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.043)
                )
                (10): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.046)
                )
                (11): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.049)
                )
                (12): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.051)
                )
                (13): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.054)
                )
                (14): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.057)
                )
                (15): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.060)
                )
                (16): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.063)
                )
                (17): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.066)
                )
                (18): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.069)
                )
                (19): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.071)
                )
                (20): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.074)
                )
                (21): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.077)
                )
                (22): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.080)
                )
                (23): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.083)
                )
                (24): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.086)
                )
                (25): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.089)
                )
                (26): ConvNeXtBlock(
                  (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
                  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=768, out_features=3072, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=3072, out_features=768, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.091)
                )
              )
            )
            (3): ConvNeXtStage(
              (downsample): Sequential(
                (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)
                (1): Conv2d(768, 1536, kernel_size=(2, 2), stride=(2, 2))
              )
              (blocks): Sequential(
                (0): ConvNeXtBlock(
                  (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)
                  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.094)
                )
                (1): ConvNeXtBlock(
                  (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)
                  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.097)
                )
                (2): ConvNeXtBlock(
                  (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)
                  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=1536, out_features=6144, bias=True)
                    (act): GELU()
                    (drop1): Dropout(p=0.0, inplace=False)
                    (norm): Identity()
                    (fc2): Linear(in_features=6144, out_features=1536, bias=True)
                    (drop2): Dropout(p=0.0, inplace=False)
                  )
                  (shortcut): Identity()
                  (drop_path): DropPath(drop_prob=0.100)
                )
              )
            )
          )
          (norm_pre): Identity()
          (head): NormMlpClassifierHead(
            (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())
            (norm): LayerNorm2d((1536,), eps=1e-06, elementwise_affine=True)
            (flatten): Flatten(start_dim=1, end_dim=-1)
            (pre_logits): Identity()
            (drop): Dropout(p=0.0, inplace=False)
            (fc): Identity()
          )
        )
        (head): Sequential(
          (mlp): Mlp(
            (fc1): Linear(in_features=1536, out_features=1536, bias=True)
            (act): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=1536, out_features=768, bias=False)
            (drop2): Dropout(p=0, inplace=False)
          )
        )
      )
      (transformer): Transformer(
        (resblocks): ModuleList(
          (0): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (1): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (2): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (3): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (4): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (5): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (6): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (7): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (8): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (9): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (10): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (11): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (12): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (13): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (14): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
          (15): ResidualAttentionBlock(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
            )
            (ls_1): Identity()
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Sequential(
              (c_fc): Linear(in_features=768, out_features=3072, bias=True)
              (gelu): GELU()
              (c_proj): Linear(in_features=3072, out_features=768, bias=True)
            )
            (ls_2): Identity()
          )
        )
      )
      (token_embedding): Embedding(49408, 768)
      (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
  (sem_seg_head): FCCLIPHead(
    (pixel_decoder): MSDeformAttnPixelDecoder(
      (input_proj): ModuleList(
        (0): Sequential(
          (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (1): Sequential(
          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
        (2): Sequential(
          (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): GroupNorm(32, 256, eps=1e-05, affine=True)
        )
      )
      (transformer): MSDeformAttnTransformerEncoderOnly(
        (encoder): MSDeformAttnTransformerEncoder(
          (layers): ModuleList(
            (0): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (1): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (2): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (3): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (4): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
            (5): MSDeformAttnTransformerEncoderLayer(
              (self_attn): MSDeformAttn(
                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)
                (attention_weights): Linear(in_features=256, out_features=96, bias=True)
                (value_proj): Linear(in_features=256, out_features=256, bias=True)
                (output_proj): Linear(in_features=256, out_features=256, bias=True)
              )
              (dropout1): Dropout(p=0.0, inplace=False)
              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (linear1): Linear(in_features=256, out_features=1024, bias=True)
              (dropout2): Dropout(p=0.0, inplace=False)
              (linear2): Linear(in_features=1024, out_features=256, bias=True)
              (dropout3): Dropout(p=0.0, inplace=False)
              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      (adapter_1): Conv2d(
        192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (layer_1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (predictor): MultiScaleMaskedTransformerDecoder(
      (pe_layer): Positional encoding PositionEmbeddingSine
          num_pos_feats: 128
          temperature: 10000
          normalize: True
          scale: 6.283185307179586
      (transformer_self_attention_layers): ModuleList(
        (0): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): SelfAttentionLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_cross_attention_layers): ModuleList(
        (0): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (2): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (3): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (4): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (5): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (6): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (7): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (8): CrossAttentionLayer(
          (multihead_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
      )
      (transformer_ffn_layers): ModuleList(
        (0): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (1): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (2): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (3): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (4): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (5): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (6): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (7): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
        (8): FFNLayer(
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (query_feat): Embedding(250, 256)
      (query_embed): Embedding(250, 256)
      (level_embed): Embedding(3, 256)
      (input_proj): ModuleList(
        (0): Sequential()
        (1): Sequential()
        (2): Sequential()
      )
      (mask_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (mask_pooling): MaskPooling()
      (_mask_pooling_proj): Sequential(
        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
      )
      (class_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=768, bias=True)
        )
      )
    )
  )
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}
      num_classes: 40
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (mask_pooling): MaskPooling()
  (void_embedding): Embedding(1, 768)
)
[04/24 19:33:29] fcclip.data.dataset_mappers.mask_former_semantic_dataset_mapper INFO: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800,), max_size=1333, sample_style='choice'), RandomFlip()]
[04/24 19:33:29] detectron2.data.datasets.coco INFO: Loaded 51846 images with semantic segmentation from /home/zpp2/ycy/datasets/LandDiscover50K/TR_Image
[04/24 19:33:29] detectron2.data.build INFO: Using training sampler TrainingSampler
[04/24 19:33:30] detectron2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[04/24 19:33:30] detectron2.data.common INFO: Serializing 51846 elements to byte tensors and concatenating them all ...
[04/24 19:33:30] detectron2.data.common INFO: Serialized dataset takes 9.01 MiB
[04/24 19:33:30] detectron2.data.build INFO: Making batched data loader with batch_size=5
[04/24 19:33:30] detectron2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from  ...
[04/24 19:33:30] fvcore.common.checkpoint INFO: No checkpoint found. Initializing model from scratch
[04/24 19:33:30] detectron2.engine.train_loop INFO: Starting training from iteration 0
[04/24 19:45:06] detectron2.engine.train_loop ERROR: Exception during training:
Traceback (most recent call last):
  File "/home/zpp2/ycy/fc-clip/detectron2/detectron2/engine/train_loop.py", line 155, in train
    self.run_step()
  File "/home/zpp2/ycy/fc-clip/detectron2/detectron2/engine/defaults.py", line 496, in run_step
    self._trainer.run_step()
  File "/home/zpp2/ycy/fc-clip/detectron2/detectron2/engine/train_loop.py", line 506, in run_step
    self.grad_scaler.scale(losses).backward()
  File "/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torch/_tensor.py", line 255, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/zpp2/anaconda3/envs/fcclip/lib/python3.8/site-packages/torch/autograd/__init__.py", line 147, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 428.00 MiB (GPU 1; 23.70 GiB total capacity; 19.95 GiB already allocated; 327.81 MiB free; 21.88 GiB reserved in total by PyTorch)
[04/24 19:45:06] detectron2.engine.hooks INFO: Overall training speed: 761 iterations in 0:11:27 (0.9031 s / it)
[04/24 19:45:06] detectron2.engine.hooks INFO: Total training time: 0:11:27 (0:00:00 on hooks)
